<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>VizAssert Artifact Evaluation</title>
  <style type="text/css">code{white-space: pre;}</style>
  <title>VizAssert Artifact Evaluation</title>
  <style>
    body { max-width: 800px; margin: 0 auto; line-height: 1.5; font-size: 18px; padding: 1em; }
    h1 { margin: 2em 0 0 1em; padding: .5em 0 0 3em; background-image: url('vizassert.svg'); background-repeat: no-repeat; background-size: 2em; height: 3em; }
  </style>
</head>
<body>
<div id="header">
<h1 class="title">VizAssert Artifact Evaluation</h1>
</div>
<div id="TOC">
<ul>
<li><a href="#downloading-the-virtual-machine">Downloading the Virtual Machine</a><ul>
<li><a href="#structure-of-the-virtual-machine">Structure of the Virtual Machine</a></li>
</ul></li>
<li><a href="#verifying-assertions-on-the-freewebsitetemplates">Verifying Assertions on the FreeWebsiteTemplates</a><ul>
<li><a href="#validating-the-experiment">Validating the experiment</a></li>
<li><a href="#verifying-a-single-assertion-fwt-combination">Verifying a single assertion-FWT combination</a></li>
</ul></li>
<li><a href="#checking-instance-and-proof-sizes-and-run-time">Checking Instance and Proof Sizes, and Run Time</a><ul>
<li><a href="#figure-8">Figure 8</a></li>
<li><a href="#figure-9">Figure 9</a></li>
<li><a href="#figure-10">Figure 10</a></li>
</ul></li>
<li><a href="#verifying-the-semantics-on-the-csswg-test-suite">Verifying the Semantics on the CSSWG Test Suite</a><ul>
<li><a href="#validating-the-experiment-1">Validating the experiment</a></li>
<li><a href="#running-an-individual-test">Running an individual test</a></li>
<li><a href="#running-further-csswg-tests">Running further CSSWG tests</a></li>
</ul></li>
<li><a href="#the-vizassert-codebase">The VizAssert Codebase</a></li>
<li><a href="#running-vizassert-on-new-inputs">Running VizAssert on new inputs</a><ul>
<li><a href="#new-assertions">New assertions</a></li>
</ul></li>
<li><a href="#setting-up-vizassert-on-another-machine">Setting up VizAssert on another machine</a><ul>
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#installing-prerequisites">Installing Prerequisites</a></li>
<li><a href="#downloading-the-csswg-test-suite-and-the-fwts">Downloading the CSSWG test suite and the FWTs</a></li>
<li><a href="#prerequisites-for-generating-plotting-code">Prerequisites for generating plotting code</a></li>
<li><a href="#debugging">Debugging</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul>
</div>
<p>VizAssert automatically checks formal properties about the possible renderings of a web page. Its main contributions are (Section 1.3):</p>
<ol style="list-style-type: decimal">
<li>A language to express visual layout properties.</li>
<li>A formalization of line height, margin collapsing, and floating layout.</li>
<li>An encoding of this formalization to SMT.</li>
<li>An implementation of this encoding into an automated tool.</li>
</ol>
<p>In support of these contributions, the VizAssert evaluation centers on two experiments:</p>
<ul>
<li>Automatically verifying or finding counterexamples for 14 industry-standard assertions on 51 professionally-designed web pages, demonstrating the flexibility of the assertion language (contribution 1) and the usability of the tool (contribution 4). Since the tool uses an SMT solver to verify counterexamples, this also demonstrates the efficiency of that encoding (contribution 3).</li>
<li>Testing that VizAssert's formalization of line height, margin collapsing, and floating layout agrees with Mozilla Firefox's, demonstrating that this formalization is correct (contribution 2).</li>
</ul>
<p>This document contains the following parts: + How to run each of the two main experiments. Each one only requires running one command. + How to verify various additional claims made in the evaluation (Section 6 in the paper). + How to run additional experiments of the reviewer's choosing (including writing new assertions, using those assertions on new web pages, and evaluating other portions of the semantics). + A brief description of the VizAssert code base, allowing the reviewer to check that it corresponds to the simplified description in the paper. + How to set up VizAssert, enabling the reviewer to recreate the provided VM or to install VizAssert elsewhere.</p>
<p><em>Note</em>: The two commands that reproduce the main experiments can take a few days to run. You may wish to run each of the main experiments overnight (or over multiple nights if you need to pause the virtual machine during the day). The provided virtual machine includes outputs, so you can examine those outputs even before re-running the experiments.</p>
<h2 id="downloading-the-virtual-machine">Downloading the Virtual Machine</h2>
<p>VizAssert is provided to the artifact reviewer in two formats:</p>
<ul>
<li>A virtual machine image</li>
<li>A tarball containing the source code</li>
</ul>
<p>The authors recommend that you use the virtual machine image. Most of this guide assumes the use of the virtual machine image. However, the final section describes how the VM image was created, enabling reviewers to verify the VM contents and permitting VizAssert to be installed on other machines. VizAssert also runs faster when not run in a VM, which may allow a closer correspondence to the numbers in the paper (which were gathered on a fast machine, run natively).</p>
<p>The virtual machine image is available from:</p>
<blockquote>
<p>https://www.dropbox.com/s/lg2gjgt2fl73lt0/vizassert-aec.ova</p>
</blockquote>
<p>The virtual machine image is based on Ubuntu 16.04 LTS with all recent updates applied. It comes with all necessary software pre-installed, all evaluation materials pre-downloaded, and a variety of preliminary steps completed.</p>
<h3 id="structure-of-the-virtual-machine">Structure of the Virtual Machine</h3>
<p>The virtual machine contains a complete checkout of the artifact in <code>~/Desktop/vizassert</code>. This directory is also accessible as the symbolic link <code>~/vizassert</code>. The virtual machine also contains the FWT and CSSWG test pages in <code>~/src</code>.</p>
<p>The VizAssert source is stored in a <code>git</code> repository. <strong>The git log and configuration can break anonymity, and reviewers should not examine it.</strong> The git repository is provided so that the authors can provide updates to scripts during artifact evaluation if reviewers discover errors or request more information, in which case <code>git pull</code> should download the update without breaking anonymity.</p>
<p>The virtual machine contains installations of Vim, Emacs, Atom, and VS Code, and the default logic has <code>sudo</code> power without a password, allowing reviewers to install additional software.</p>
<p>The virtual machine image logs directly into a GUI session for the <code>vizassert</code> user. Authors on slower machines may wish to log out, and log back into the <code>vizassert</code> user (password <code>vizassert</code>) using the &quot;Gnome Classic (metacity)&quot; session, which has fewer animations and visual details.</p>
<p>The VM requests 2GB of memory and 1 processor core by default. If the reviewer's machine has more memory and processor cores, these defaults can be increased. (In VirtualBox, while the VM is powered off, in the &quot;Machine&quot; menu select &quot;Settings&quot; and adjust values in the &quot;System&quot; tab.)</p>
<h2 id="verifying-assertions-on-the-freewebsitetemplates">Verifying Assertions on the FreeWebsiteTemplates</h2>
<p>The first major experiment in the VizAssert paper is an evaluation of VizAssert on 51 professionally-designed web pages from the FreeWebsiteTemplates community (FWTs).</p>
<p>Due to differing machines and VM overhead, the setup step may have left the reviewer with slightly more or slightly fewer than 51 FWTs in the file <code>bench/fwt.working.rkt</code>. The VM has 46 FWTs selected (since the VM is slower than the paper evaluation computer and thus more timeouts were hit). The authors believe that as few as 40 or as many as 60 FWTs supported would represent substantially similar results to those in the paper.</p>
<p>The to test each FWT against all 8 general assertions, and then test the additional 6 specific assertions, run the command:</p>
<pre><code>make reports/general.html reports/specific.html</code></pre>
<p>This will generate <code>reports/general.html</code> with the results of running the 8 general assertions on all 51 FWTs and <code>reports/specific.html</code> with the results of running the 6 specific assertions each on their particular FWT. (If you installed VizAssert yourself you may need to change the <code>FWT_PATH</code> Make variable.)</p>
<p>This command will run for several hours or perhaps up to a day. The VM contains pre-generated versions of these two files, which the reviewer can examine while the above command is running. (The reviewer can then check, once the commands complete, that the reproduced results are similar.)</p>
<p>Each of these pages will contain two parts: a summary table at the top (listing the successful verifications &quot;success&quot;, false positives &quot;fail&quot;, timeouts &quot;timeout&quot;, and true positives &quot;expected&quot;); and a detailed table below (listing each FWT and an icon for every assertion).</p>
<p>The results can be compared to Table 2 in the paper. Due to variation between machines, some numbers, especially the number of timeouts, may change somewhat from run to run. The authors would consider variations with as many as 30 timeouts expected given the expected slowdown from running inside a virtual machine; the paper's evaluation was done on a very fast machine.</p>
<p>Note that true positives are verified by hand (all manual verifications are recorded in <code>bench/fwt/expected.sexp</code>). If you set up VizAssert yourself rather than using the provided virtual machine, setup might have chosen a slightly different set of supported FWTs, so you may see a different number of &quot;false positives&quot;.</p>
<h3 id="validating-the-experiment">Validating the experiment</h3>
<p>This section describes in more detail the commands being executed, so the reviewer can be certain that the experiment reproduced corresponds to the text of the paper.</p>
<p>The <code>make</code> command above executes <code>racket src/report.rkt</code>, which runs VizAssert on multiple input files and produces HTML reports. This command takes multiple arguments.</p>
<p>To produce <code>reports/general.html</code>, it is run in <code>assertions</code> mode with two arguments:</p>
<ul>
<li><code>bench/assertions/general.vizassert</code>, a file containing the 8 general assertions. This file can be read by the reviewers to ensure correspondence to visual logic, as described in the paper, and that the assertions implement the described guidelines. The supplementary material in Appendix A describes details of the formalization. The assertions are written in that appendix with inline function definitions and various shorthand; thus the assertions in this file differ in cosmetic ways from Appendix A.</li>
<li><code>bench/fwt.working.rkt</code>, a file containing the 51 FWTs selected as fitting within VizAssert's supported subset of CSS.</li>
</ul>
<p>To produce <code>reports/specific.html</code>, <code>report.rkt</code> is run in directory <code>specific-assertions</code> and takes an additional argument, <code>bench/fwt/specific.sexp</code>. This file specifies FWT-assertion pairs. These assertions are drawn from <code>bench/assertions/specific.vizassert</code>.</p>
<p>Several additional arguments are passed:</p>
<ul>
<li><code>--expected bench/fwt/expected.sexp</code> names the file containing manually-verified true positives.</li>
<li><code>--timeout 1800</code> sets the timeout to half an hour.</li>
<li><code>--show-all</code> shows successful verifications on the report.</li>
<li><code>-o reports/general.html</code> sets the output name.</li>
</ul>
<p>The reviewer can pass additional flags using the <code>FLAGS</code> make variable, as in <code>make FLAGS=&quot;--threads 6&quot;</code>. The reviewer may wish to set the <code>--threads</code> option, which allows the reports to be generated in parallel. The paper evaluation server ran with 6 threads.</p>
<h3 id="verifying-a-single-assertion-fwt-combination">Verifying a single assertion-FWT combination</h3>
<p>To verify a true or false positive, a single assertion-FWT combination can be run. The command to do this is:</p>
<pre><code>racket src/run.rkt assertion bench/assertions/general.vizassert \
  &lt;assertion-name&gt; bench/fwt.working.rkt &lt;document-name&gt;</code></pre>
<p>(To run one of the specific assertions, change <code>general.vizassert</code> to <code>specific.vizassert</code>.)</p>
<p>This will print basic timing information and then output a counterexample rendering followed by the rendering parameters for this rendering. The counterexample rendering will also have (in red if your terminal supports it) a tag such as <code>:cex b</code> on one of the boxes in the rendering. Here <code>b</code> is the variable which, when bound to that box, causes the assertion to fail.</p>
<p>The best way to verify whether the counterexample is a true or false positive is to open the responsible FWT in a browser. This is tricky. Suppose you are verifying a failed assertion on <code>doc-080</code>.</p>
<ol style="list-style-type: decimal">
<li>First, search <code>fwt.working.rkt</code> for <code>define-problem doc-080</code>. Two lines down will be a path such as <code>file:///tmp/vizassert-get-bench.fKaFdo.zip/rehabilitation-yoga/rehabilitation-yoga/upload/index.html</code>. Extract from this path the name of this FWT, in this case <code>rehabilitation-yoga</code>.</li>
<li>Run <code>make /tmp/rehabilitation-yoga/</code>. The final slash is necessary.</li>
<li>Change directory to <code>/tmp/rehabilitation-yoga/</code> and run <code>find -name index.html</code> to find the main page of the site.</li>
<li>Open this file in Firefox.</li>
</ol>
<p>To find, in the browser, HTML element responsible for the assertion failing:</p>
<ul>
<li>The box with the <code>:cex b</code> tag will likely also have a tag such as <code>:elt 12</code>. (If the box in question does not, one of its siblings or ancestors will.) Remember this number.</li>
<li>Search <code>fwt.working.rkt</code> for <code>define-document doc-080</code></li>
<li>Search <em>from that point forward</em> for <code>:num 12</code>.</li>
<li>That identifies the HTML element in VizAssert's encoding of the HTML. The corresponding element can be identified in the browser.</li>
</ul>
<p>VizAssert could do most of these steps automatically if it had tighter integration into the user's workflow, and since the paper we have made some of these changes. However, these changes are not included in this artifact, in order to faithfully reflect the tool described in the paper.</p>
<h2 id="checking-instance-and-proof-sizes-and-run-time">Checking Instance and Proof Sizes, and Run Time</h2>
<p>The artifact contains the code necessary to regenerate Figures 8, 9, and 10 of the paper. These plots are generated as TeX code and then rendered to a PDF. The raw data for the plots is also available, and code is included to reproduce the broad summary statistics given in Section 6.1 that run parallel to Figure 8.</p>
<p>Each plot is generated from a Python script in <code>aec/</code>. Generated PDFs can be viewed within the VM using the <code>evince</code> PDF reader.</p>
<h3 id="figure-8">Figure 8</h3>
<p>Figure 8 examines the captured web pages and simply counts the number of rules, boxes, and elements. Rough statistics about these sizes can be found by running:</p>
<pre><code>racket aec/histogram.rkt bench/fwt.working.rkt</code></pre>
<p>This outputs the averages, IQRs, and total ranges for the sets of elements, boxes, and rules. Also output are data for drawing a histogram, such as:</p>
<pre><code>Histogram: &quot;min&quot;: 53, &quot;max&quot;:468, &quot;interval&quot;: 42, &quot;counts&quot;: [10,39,29,8,6,3,3,1,1,1]</code></pre>
<p>This represents a histogram that ranges from 53 to 468 with buckets of size 42, where the first bucket (53–95) contains 10 entries, the second contains 39 entries, and so on.</p>
<p>The histograms can be rendered with:</p>
<pre><code>make reports/hists.pdf</code></pre>
<p>Note that the given command uses the supported web pages, which may be a slightly different set than on the paper evaluation server. However, the result should be roughly comparable.</p>
<h3 id="figure-9">Figure 9</h3>
<p>Figure 9 tracks the size of instances and unsatisfiability cores. These are output by VizAssert every time it verifies a web page, and are recorded in <code>reports/general.txt</code>. Each log entry looks something like this:</p>
<pre><code>bench/fwt.working.rkt   doc-021 interactive-onscreen
[   0.001s] Read 1 documents with 75 elements, 186 boxes, and 87 rules
[   1.789s] Produced 7757 constraints of 82108 terms
[  12.274s] Prepared 28623 constraints of 677318 terms
[  35.239s] Found core with 11920 constraints
success</code></pre>
<p>In this log entry, the instance has 677,318 terms, and the proof size is 11,920 constraints.</p>
<p>The CDFs of Figure 9 can be regenerated with:</p>
<pre><code>make reports/insts.pdf</code></pre>
<p>Like with Figure 8, this may not exactly match the paper, but should be similar.</p>
<h3 id="figure-10">Figure 10</h3>
<p>Figure 10 tracks the time needed to verify assertions on the FWT pages. This figure draws data from <code>general.json</code>.</p>
<p>The CDFs of Figure 10 can be regenerated with:</p>
<pre><code>make reports/runtime.pdf</code></pre>
<p>Just as Figure 8 and 9, the generated figure may not exactly match the paper. In particular, the slowdown due to virtualization overhead will likely shift the plot to the right, compared to the plot in the paper.</p>
<h2 id="verifying-the-semantics-on-the-csswg-test-suite">Verifying the Semantics on the CSSWG Test Suite</h2>
<p>The second major VizAssert experiment tests that VizAssert's formalization of line height, margin collapsing, and floating layout is correct. The experiment can be run with:</p>
<pre><code>make reports/csswg.html</code></pre>
<p>Like the first experiment, the <code>FLAGS</code> Make variable can be passed to run this experiment with multiple threads. With a single thread it takes several hours to complete. (As in <code>make FLAGS=&quot;--threads 6&quot;</code>.)</p>
<p>This command generates <code>reports/csswg.html</code>, which the reviewer can open in a web browser and compare to Table 3, columns 3 and 4, of the paper. The &quot;success&quot; column on the report corresponds to the &quot;Pass&quot; column in the paper, while &quot;unsupported&quot; and &quot;timeout&quot; are added together to compute the &quot;Unsup&quot; column.</p>
<p>As with the other experiments, different computer setups may cause slightly more or slightly fewer tests to time out than on the paper evaluation machine. The authors would consider as few as 900 or as many as 924 passing tests to substantially correspond to the numbers in Table 3.</p>
<p>Note that unlike the prior experiment, the reviewer will see few green check marks (or red cross marks) on this report, because this report is generated without the <code>--show-all</code> flag set, and thus hides passing tests. The reviewer will, however, see a new section listing &quot;unsupported features&quot;. This table is generated during capturing (see the &quot;Setting up VizAssert&quot; section of this README) and indicates features used by the page. It is not an exhaustive list of all supported or unsupported features, but it does provide some guidance explaining why tests end up in the &quot;unsupported&quot; column. (In this table, the &quot;# Necessary&quot; column lists the number of failing tests that are unsupported and use this feature, while the &quot;# Blocking&quot; column lists the number of failing tests that are unsupported and use <em>only</em> this feature.)</p>
<p>As described in the paper, four tests are expected failures (and so do not count as failures in the report). These are <code>doc-13</code>, <code>doc-17</code>, <code>doc-19</code>, and <code>doc-21</code> in <code>floats.rkt</code>, or, in the CSSWG tests (located in <code>~/src/csswg</code> in the VM):</p>
<ul>
<li><code>css/CSS2/floats/floats-rule3-outside-left-002.xht</code></li>
<li><code>css/CSS2/floats/floats-rule3-outside-right-002.xht</code></li>
<li><code>css/CSS2/floats/floats-rule7-outside-left-001.xht</code></li>
<li><code>css/CSS2/floats/floats-rule7-outside-right-001.xht</code></li>
</ul>
<p>To verify that Firefox's rendering is incorrect, and thus that VizAssert's failure is expected, the reviewer can add <code>-ref</code> just before the <code>.xht</code> to any of those file names to see the test's reference example; Firefox will render the test and the reference differently, demonstrating that its rendering is incorrect. (The paper cites a relevant bug report.)</p>
<h3 id="validating-the-experiment-1">Validating the experiment</h3>
<p>This section describes in more detail the commands being executed, so the reviewer can verify that the experiment reproduced corresponds to the text of the paper.</p>
<p>The <code>make</code> command above uses information in the <code>Makefile</code> to execute the experiment. It executes <code>report.rkt</code> in the <code>regression</code> mode, which tests whether VizAssert accepts Firefox's rendering. In addition, it passes the arguments:</p>
<ul>
<li><code>--index bench/css/index.json</code>, which points VizAssert to a JSON file giving the section of the standard any particular unit test was drawn from, according to the <a href="http://test.csswg.org/suites/css2.1/20110323/html4/toc.html">CSSWG's site</a>.</li>
<li><code>--expected bench/css/expected.sexp</code> describes expected failures</li>
<li><code>--sections ...</code> lists all of the sections of the CSS standard tested in this experiment.</li>
</ul>
<h3 id="running-an-individual-test">Running an individual test</h3>
<p>Unlike the FWT tests, the CSSWG tests are organized into multiple files, reflecting the structure of the test suite. For example, most of the tests for floating layout are located in the <code>floats</code> and <code>floats-clear</code> collections. Each collection's tests are numbered sequentially, but a collection also contains tests corresponding to multiple sections of the standard. (The authors don't know why the CSSWG uses this organization scheme.) To reference a test, you must know both its collection (like <code>floats</code>) and the test name (like <code>doc-13</code>).</p>
<p>An individual test can be reproduced with:</p>
<pre><code>racket src/run.rkt accept bench/css/&lt;collection&gt;.rkt &lt;test&gt;</code></pre>
<p>This will either print &quot;Accepted.&quot;, which means that VizAssert agreed that Firefox's rendering was correct, or &quot;Rejected.&quot; followed by a visualization of the unsatisfiability core of VizAssert's rejection of Firefox's rendering. On the provided tests, this unsatisfiability core is rarely useful, because the underlying reason for the failure is more often &quot;VizAssert doesn't understand tables&quot; and less often &quot;this element shouldn't have width 100px&quot;.</p>
<p>To produce some rendering that VizAssert believes is valid, without using any information about Firefox's rendering, run:</p>
<pre><code>racket src/run.rkt render -d bench/css/&lt;collection&gt;.rkt &lt;test&gt;</code></pre>
<p>VizAssert's can accept multiple renderings, because VizAssert's CSS semantics are mildly non-deterministic. However, the semantics are fairly tight, as demonstrated by the first experiment: if too many renderings were accepted, VizAssert would produce many false positives. In some cases, VizAssert may print &quot;Failed to render&quot;, indicating that no valid renderings could be found. This indicates a bug.</p>
<p>The <code>-d</code> flag places VizAssert in &quot;debug mode&quot;, whose most important consequence is that VizAssert does not apply &quot;fuzzing&quot; to font measurements (see Footnote 13 in the paper). It also causes VizAssert to write out its SMT query to <code>/tmp/out.z3</code>. The reviewer may read that file to see the character of queries produced by VizAssert.</p>
<h3 id="running-further-csswg-tests">Running further CSSWG tests</h3>
<p>The reviewer can run:</p>
<pre><code>make reports/csswg-extra.html</code></pre>
<p>This generates a larger report that evaluates VizAssert not only on the sections described in the paper but also on a variety of other CSSWG tests. The authors stress that these tests apply to aspects of the CSS semantics not discussed in the paper, and do not evaluate any of the paper's core contributions.</p>
<p>The reviewer will likely see 100-200 test failures out of 5000 tests. The authors have not examined them all. Some may be due to errors in the semantics, others are due to errors in the capture script, and some may represent expected failures (that is, Firefox bugs). Note also that some of these tests are intended to be applied only on machines with certain properties, such as certain font setups, and so are simply not applicable. This report does not make such distinctions.</p>
<p>Reviewers who are interested in exploring this broader scope of tests are encouraged to use the <code>accept</code> and <code>render</code> commands above to explore some of the failing tests.</p>
<p>The relatively low rate of test failures (roughly 2% of tests) suggests that even in the unevaluated portions of the CSS standard, VizAssert's semantics is largely correct. Neither could VizAssert have successfully passed the two main experiments without largely correct semantics for the core of CSS.</p>
<h2 id="the-vizassert-codebase">The VizAssert Codebase</h2>
<p>VizAssert is a total of approximately 7000 lines of code in <code>src/</code>. This code formalizes CSS, including the formalizations of line height, margin collapsing, and floating layout described in the paper; generates SMT queries for web pages using this formalization; and interacts with the solver.</p>
<p>Visual logic is implemented by <code>assertions.rkt</code>.</p>
<p>The constraint generator lives in <code>main.rkt</code> and <code>frontend.rkt</code> and refers to functions defined in the formalization.</p>
<p>The formalization lives, by and large, in <code>spec/</code>. <code>spec/utils.rkt</code> defines the element and box types, and <code>spec/tree.rkt</code> describes functions that set up both tree structures. <code>spec/layout.rkt</code> formalizes the CSS 2.1 layout algorithm. <code>spec/float.rkt</code> implements the exclusion zone data structure, and includes formal properties describing its correctness. (These properties can be checked with <code>raco test</code>.) <code>spec/css-properties.rkt</code> houses the implementation of CSS properties, <code>spec/compute-style.rkt</code> of inheritance, and <code>selectors.rkt</code> of selectors and cascading. Other files in <code>spec/</code> contain utility functions.</p>
<p>Constraints are simplified by a Z3 optimizer in <code>z3o.rkt</code>. Interaction with the solver is in <code>z3.rkt</code>.</p>
<p><code>run.rkt</code> and <code>report.rkt</code> implement the VizAssert frontend.</p>
<p>The various other files define various data structures and helper functions.</p>
<p>As this paper emphasizes the formalizations of line height, margin collapsing, and floating layout, reviewers may want to directly look at the code implementing these features. All three features are implemented in <code>spec/layout.rkt</code> and, due to the need to implement the features in a way that makes solving them efficient, are more fragmented than the description in the paper.</p>
<p>Line height computation is implemented in lines 879–896 (which computes the position and height of lines) and supported by lines 751–770 and 825–833 (which propagate the <code>above-baseline</code> and <code>below-baseline</code> properties for inline and text boxes). In this code, <code>b</code> generally refers to the particular box whose values are being computed, <code>l</code> is its last child, and <code>v</code> is its previous sibling. Note that the full implementation is more complex than the paper sketch, due to details like zero-height line boxes that were elided from the paper for simplicity.</p>
<p>Margin collapsing is implemented in lines 224–270. It follows the sketch in the paper; the variable <code>mtp</code> and <code>mtn</code> are the variables <code>mt+</code> and <code>mt-</code> (likewise <code>mbp</code>/<code>mbn</code>), <code>mtn-up</code> the variable <code>mt↑</code>, and <code>mb-clear</code> the boolean <code>mb?</code>. The <code>b</code>, <code>l</code>, and <code>v</code> variables are as above, with <code>f</code> for the first child and <code>n</code> for the next child. This implementation of margin collapsing is supported by lines 121–131, which uses the results of the margin collapsing code to implement placement for in-flow block boxes.</p>
<p>Floating layout is implemented in lines 639–659 (which place floating boxes and update exclusion zones) and supported by lines 858–871 (which places line boxes to avoid floats). Exclusion zones themselves are implemented in <code>spec/floats.rkt</code>.</p>
<p>These implementations cannot be easily read on their own, since they tie into the larger formalization of floating layout and also since they include special cases not discussed in the paper yet important for correctly implementing the relevant behavior. (For example, the paper simply did not have room to discuss clearance, an important component of CSS layout that is intimately related to floating layout.) However, we hope that reviewers will be able to see some rough correspondence to the descriptions in the paper, at least relating common variables and a rough understanding of data flow.</p>
<h2 id="running-vizassert-on-new-inputs">Running VizAssert on new inputs</h2>
<p>Reviewers may run VizAssert on new web pages and new assertions. Keep in mind that VizAssert's formalization of CSS is limited to a subset of the full features and that visual logic does not at the moment have comprehensive and comprehensible error reporting.</p>
<p>To run VizAssert, one fundamentally needs two things: a web page and an assertion. Reviewers may write a web page or choose any public page on the Internet; VizAssert can be used either on files on disk or on HTTP-accessible (but not HTTPS-accessible) pages. VizAssert can have surprising and unpredictable results on JavaScript-heavy web pages, because the capture scripts that import web pages into VizAssert's input format are themselves written in JavaScript, and will run at an unpredictable time and may conflict in their use of global variables. The authors recommend reviewers choose pages with minimal JavaScript.</p>
<p>Reviewers who wish to run VizAssert on JavaScript-heavy or HTTPS-accessible pages can use their browser's &quot;save page&quot; feature to save a static version of the page on disk. (Some browsers will save and later run JavaScript on such pages; we recommend simply deleting the relevant JavaScript file if it causes problems.)</p>
<p>Once a page is chosen, run:</p>
<pre><code>python2 get_bench.py --name test &lt;url&gt;</code></pre>
<p>Here, <code>test</code> is any name; the script will create <code>bench/test.rkt</code> by going to the chosen URL (it may also be a Unix path), running the capture script, and recording the results. Multiple URLs can be given, but we do not recommend reviewers do this.</p>
<p>The resulting <code>bench/test.rkt</code> file will contain a single VizAssert input, named <code>doc-1</code>.</p>
<p>Given an input file, run:</p>
<pre><code>racket src/run.rkt accept bench/test.rkt doc-1</code></pre>
<p>You should see the text <code>Accepted</code> if your page is supported by VizAssert. If it is not, you may see error messages at various stages of the process. Unfortunately, at the moment these error messages are not particularly clear.</p>
<p>Note that this process can take a long time for large, or even moderate-sized pages. VizAssert cannot be run, for example, on the CNN or New York Times front pages—these pages are simply too big. The authors recommend trying the reviewer's personal web page (of course, it will be difficult to assist in debugging given the requirements of anonymity.)</p>
<p>Given a supported input file and an assertion, run:</p>
<pre><code>racket src/run.rkt assertion &lt;assertion-file&gt; &lt;assertion-name&gt; bench/test.rkt doc-1</code></pre>
<p>This asks VizAssert to verify that the page satisfies the assertion. If this page cannot be verified, VizAssert will produce a counterexample. These counterexamples were described in detail when discussing the first experiment.</p>
<h3 id="new-assertions">New assertions</h3>
<p>You can re-use an existing assertion or create your own.</p>
<p>A new assertion can be written by creating an empty file and filling it with this template:</p>
<pre><code>(define-test (&lt;name&gt; &lt;forall-quantified-vars&gt; ...)
  &lt;assertion&gt;)</code></pre>
<p>For example, the simplest of the 8 general-purpose assertions is defined by:</p>
<pre><code>(define-test (no-horizontal-scroll b)
  (&lt;= (right b) (right root)))</code></pre>
<p>Consult <code>general.vizassert</code> for more and more complex examples.</p>
<p>The the assertion language is defined in <code>src/assertions.rkt</code> and largely follows the visual logic in the paper. It incorporates some but not all of the shorthands used in the supplementary Appendix A.</p>
<p>Note that though we encourage reviewers to experiment with VizAssert, specifying new assertions can be tricky and it may be difficult for us to help troubleshoot reviewers' web pages and assertions.</p>
<h2 id="setting-up-vizassert-on-another-machine">Setting up VizAssert on another machine</h2>
<p>Setting up VizAssert on your own machine is labor-intensive. The authors will not necessarily be able help debug installation problems on all systems.</p>
<h3 id="prerequisites">Prerequisites</h3>
<p>VizAssert is written in Racket, with bits of glue code written in JavaScript and Python. It also uses the Z3 SMT solver, the Firefox web browser, and the Selenium Python package. Finally, its associated scripts also involve minor prerequisites: the <code>xvfb-run</code>, <code>curl</code>, and <code>fuse-zip</code> commands, and (for some some versions of Selenium and Firefox) the <code>geckodriver</code> binary. The PIP package manager is also recommended. This section guides the reviewer through installing all of these prerequisites.</p>
<p>VizAssert runs on several different Linux distributions, macOS, and Windows. However, this section describes installation instructions only on Linux; you will have to modify them for other operating systems. Note that several of the minor prerequisites are not available on macOS and Windows. The following instructions describe workarounds where available, but the artifact is not optimized for this use case. The VM uses Ubuntu 16.04.3 LTS.</p>
<p>Users have reported that Retina and other Hi-DPI displays cause problems when capturing pages in VizAssert. The authors have not been able to confirm these reports. Reviewers on devices with Hi-DPI screens may want to use the VM, where the capture step has already been completed on a device with a Lo-DPI screen.</p>
<h3 id="installing-prerequisites">Installing Prerequisites</h3>
<p>Racket, Python, Z3, and Firefox can all be installed from their respective websites or from package managers. The version requirements are:</p>
<ul>
<li>Racket 6.7 and up has been observed to work, and Racket 6.3+ likely will too. However, Racket 6.10 has a bug in its IO subsystem that prevents it from interacting properly with Z3; it is fixed in Racket 6.10.1. The VM uses Racket 6.10.1, while the paper's evaluation environment used Racket 6.7.</li>
<li>Python 2.7 is required. The VM uses 2.7.12, while the paper's evaluation environment used Python 2.7.13.</li>
<li>Z3 4.4.1, 4.5.1, and 4.6.0 have all been tested. Likely any version will work; however, some versions have bugs that cause spurious error messages. The authors do not recommending building pre-release Z3 versions from source; they are often unstable. The VM uses 4.6.0, while the paper's evaluation environment used 4.5.1.</li>
<li>Firefox 52 or later should all work. Earlier versions will also work, but used a different protocol for communicating with Selenium, so need to be carefully matched to Selenium versions. The VM uses Firefox 58.0.2, while the paper's evaluation environment used Firefox 52.5.2.</li>
</ul>
<p>Note that Racket, Python, Z3, Firefox, and the utilities below must all be available on your PATH (as <code>racket</code>, <code>python2</code>, <code>z3</code>, <code>firefox</code>, etc.). On Windows, modifying one's PATH is complicated; <a href="https://helpdeskgeek.com/windows-10/add-windows-path-environment-variable/">this guide</a> may be helpful.</p>
<p>Selenium is best installed from PIP (the Python package manager). PIP can be installed from its website or from a package manager (any version should work). Remember to use the version of PIP that corresponds to your Python 2.7 installation (if you have multiple Python versions on your machine). Selenium can then be installed with <code>sudo pip install selenium</code>. (If recent versions of Selenium are available through a package manager, that can also work, but often the available version is too old.) The VM uses Selenium version 3.9.0 while the paper's evaluation environment used Selenium 2.53.2.</p>
<p>For Selenium to communicate properly with Firefox, you may need to install a binary named <code>geckodriver</code> and place it on your path. This binary is sometimes available in package managers, named <code>geckodriver</code>, <code>firefoxdriver</code>, or <code>firefox-webdriver</code>. It may also be part of the Firefox installation. You can check by executing, in a Python REPL (<code>python2</code>),</p>
<pre><code>import selenium.webdriver
f = selenium.webdriver.Firefox()</code></pre>
<p>Note that this command starts Firefox—if no GUI is available (if you are on a remote machine over SSH and are not forwarding X), it will fail. If this command issues an exception complaining about not finding a <code>geckodriver</code> or <code>webdriver</code> binary, then you will need to install <code>geckodriver</code>. It can be found on Github; it comes in a tarball which decompresses into a single binary. This binary must be placed on your <code>PATH</code>.</p>
<p>The VizAssert scripting also uses the <code>fuse-zip</code>, <code>xvfb-run</code>, and <code>curl</code> commands. <code>fuse-zip</code> allows mounting ZIP files as a file system; the scripting uses this because the FWT web pages compress really well, and decompressing them would make the VM download much bigger and slow down the scripts by hitting disk much more often. It can be installed from package managers, but it will likely require a reboot and possibly some group manipulations to allow the user to access FUSE. The authors ask the reviewer to consult their distribution's guidelines on FUSE. If <code>fuse-zip</code> is not present (e.g., on Windows or macOS), delete the line that begins with <code>hash fuse-zip</code> at the top of <code>get-all.sh</code> and then replace <code>fuse-zip -r &quot;$FILE&quot; $tmpdir/$NAME</code> with <code>unzip &quot;$FILE&quot; -d $tmpdir/$NAME</code> and the line <code>fusermount -u $tmpdir/$NAME</code> with <code>rm -rf $tmpdir/$NAME</code>.</p>
<p><code>xvfb-run</code> allows rendering GUI applications to a virtual screen; the VizAssert scripts use this to run Firefox without opening a physical window. This script can be installed from package managers (it may be in a package called <code>xvfb</code> or <code>xvfb-xorg</code> or similar). If <code>xvfb-run</code> is not available, delete the code <code>xvfb-run -a -s '-screen 0 1920x1080x24'</code> from <code>bench/fwt/get.sh</code>, <code>bench/fwt/get_all.sh</code>, and <code>bench/css/get.sh</code>. This just causes <code>xvfb-run</code> not to be called. (The named files are just all uses of <code>xvfb-run</code> in the VizAssert codebase.) Note that removing the <code>xvfb-run</code> invocation means that a different screen size is used to capture web pages (see below). This is unlikely to change the results, but could in principle lead to a few tests no longer passing.</p>
<p><code>curl</code> is available online from its website and runs on pretty much every system.</p>
<h3 id="downloading-the-csswg-test-suite-and-the-fwts">Downloading the CSSWG test suite and the FWTs</h3>
<p>Choose a directory <code>DIR</code> to obtain the CSSWG test suite and the FWTs, then obtain them by running (the VM uses <code>~/src/</code>):</p>
<pre><code>mkdir DIR
cd DIR
git clone https://github.com/w3c/web-platform-tests.git
mkdir fwt
cd &lt;source directory&gt;
bash aec/download-fwts.sh &quot;DIR/fwt/&quot;</code></pre>
<p>The <code>aec/download-fwts.sh</code> script will take minutes to an hour, depending on your connection. In total, this step requires about 2.3GB of storage.</p>
<p>The <code>download-fwts.sh</code> script contains the list of 100 FWTs downloaded. These are the 100 most recent FWTs, as can be verified on the <a href="https://freewebsitetemplates.com">FWT website</a>.</p>
<p>Next, <em>capture</em> the CSSWG unit tests and the FWT pages. Capturing converts these pages to VizAssert's representation of HTML and CSS, plus capturing the shape of the <em>render tree</em> (or <em>box tree</em>) from Firefox. Capturing uses a Python script to remote-control a Firefox instance to run a large and complex JavaScript file (<code>get_bench.js</code>) that extracts rendering information from Firefox. To capture websites, run:</p>
<pre><code>bash aec/capture.sh &quot;DIR/fwt/&quot; &quot;DIR/web-platform-tests/&quot;</code></pre>
<p>This will produce the file <code>bench/fwt.rkt</code> and several files <code>bench/css/*.rkt</code>. Comments in those files indicate which captured entry corresponds to which test file. (The reviewer is free to verify that the capturing behaved correctly.)</p>
<p>Note that capture script only captures files named <code>index.html</code>; each FWT contains a single file with this name. This name is the customary name for the main page of a website. FWTs contain other pages as well, but these are not captured and thus not used for the remaining experiments.</p>
<h4 id="selecting-supported-fwts">Selecting supported FWTs</h4>
<p>VizAssert does not support all 100 FWTs. First, some are too large and time out; second, some use unsupported portions of CSS, like the <code>vertical-align</code> property or the <code>:before</code> and <code>:after</code> selector; third, some FWTs may trigger bugs in the capture script or the VizAssert semantics. (Experiment two shows that these bugs do not exist in the portions of the semantics that are contributions of this paper.) The authors have already fixed some of these semantics bugs, but these fixes are not included in the code submitted for artifact evaluation, to ensure that the artifact corresponds to the submitted paper.</p>
<p>To select the supported FWTs, check that for a particular screen size, VizAssert &quot;accepts&quot; Firefox's rendering by executing:</p>
<pre><code>make bench/fwt.working.rkt</code></pre>
<p>This step will take from a few hours to potentially a day or two depending on your machine, the available RAM, and possibly the version of Z3 used. This step asks VizAssert to check, for each FWT, whether Firefox's rendering (as captured in the previous step) is allowed by its semantics, with a timeout of 15 minutes. (Verifying assertions takes longer than checking a particular rendering, so this timeout is lower than the timeout of 30 minutes for verifying assertions.)</p>
<p>VizAssert produces the file <code>reports/fwt.html</code> describing the results, which the reviewer should examine. The main table, with colored check marks and Xs, should contain many green checkmarks (the table at the top should list the number, in the &quot;success&quot; column). If, instead, it only has white-on-black exclamation marks, some part of the setup did not work, and VizAssert is running into errors. (See below.) On the paper evaluation machine, 51 FWTs passed this stage; reviewers may see a different number due to different machine speed (affecting timeouts) or different fonts (affecting the rendering). The authors believe that numbers in the 40–60 range are sufficiently close to achieve comparable results in the remainder of the artifact evaluation.</p>
<p>Then, using the data in this file (and its associated JSON version <code>reports/fwt.json</code>) it produces a filtered version <code>bench/fwt.working.rkt</code> with only those FWTs that passed the check.</p>
<h3 id="prerequisites-for-generating-plotting-code">Prerequisites for generating plotting code</h3>
<p>Python 3.5+ is required to run the plotting code. The VM uses Python 3.5.2, while the paper's evaluation environment used Python 3.6.3.</p>
<p>A TeX installation is also required; any modern TeX installation that includes the <code>tikz</code> package should suffice; the VM uses TeXLive 2015, while the paper's evaluation environment used TexLive 2016.</p>
<h3 id="debugging">Debugging</h3>
<p>VizAssert can be run on an individual FWT with this command:</p>
<pre><code>racket src/run.rkt accept bench/fwt.rkt doc-080</code></pre>
<p>Here, <code>doc-080</code> names an FWT, which are numbered from <code>doc-001</code> to <code>doc-100</code>. (<code>doc-080</code> is the yoga website in Figure 7.)</p>
<p>Running individual FWTs may help reviewers debug problems they face setting up Racket, Z3, or the other components above.</p>
<h3 id="conclusion">Conclusion</h3>
<p>After following these steps, the reviewer should have the file <code>bench/fwt.working.rkt</code> and several files <code>bench/css/*.rkt</code>, plus a working installation of VizAssert. These are sufficient to move on to artifact evaluation.</p>
</body>
</html>
